<html>
<head>
    <title>Try it and see!</title>
    <meta charset='UTF-8'>
    <meta content='width=device-width, initial-scale=1' name='viewport'/>

    <meta name='description' content='Gregory Gundersen is a PhD candidate at Princeton.'>
    <meta name='keywords' content='software engineering, metacognition, research, applied research'>
    <meta name='author' content='Gregory Gundersen'>

    <link href='/css/blog.css' rel='stylesheet'/>
    <link href='/css/trac.css' rel='stylesheet'/>
    <link href='/css/markdown.css' rel='stylesheet'/>

    <script type='text/x-mathjax-config'>
MathJax.Hub.Config({
  jax: ['input/TeX', 'output/HTML-CSS'],
  tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$']],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
    extensions: ['color.js']
  },
  messageStyle: 'none',
  'HTML-CSS': { preferredFont: 'TeX', availableFonts: ['STIX','TeX'] }
});
</script>

<script src='//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML' type='text/javascript'></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
</head>
<body>
<div class='content'>
    <div class='nav'>
    <ul class='wrap'>
        <li><a href='/'>Home</a></li>
        <li><a href='/blog'>Blog</a></li>
        <li><a href='/feed.xml'>RSS</a></li>
    </ul>
</div>
    <div class='front-matter'>
        <div class='wrap'>
            <h1>Try it and see!</h1>
            <h4>Getting better as a SWE, analysis paralysis and why it's so hard for me to accept advice</h4>
            <div class='bylines'>
                <div class='byline'>
                    <h3>Published</h3>
                    <p>04 January 2024</p>
                </div>
            </div>
            <div class='clear'></div>
        </div>
    </div>
    <div class='wrap article'>
        <p>I asked my friend, who is much further in his SWE career, how to improve as a software engineer.</p>

<p>He started with just, <strong>scale</strong>, that I should build projects at ‚Äúlarge-scale‚Äù. ‚ÄúLarge-scale‚Äù, as I later found out, generally means that your service needs to support high loads, respond really fast, or process extremely large amounts of data (sometimes you need to do more than one of these things). To meet these requirements, you make guarantees about your service through metrics like availability, latency or throughput. I initially thought that this was something you only really encounter in infra teams, but my friend disagreed. It‚Äôs something that can be observed through any project <strong>at scale</strong>.</p>

<p>After a lot of reflection, I realized that he was right! I even had a similar learning experience: scaling <a href="https://github.com/dhrumilp15/haystackfs">Haystackfs</a>, my file search engine for discord, to 380+ servers and 1M+ files.</p>

<p>When I first built the search engine, I tried spinning up ElasticSearch indices on my server. That worked for a few requests, until it would run out of memory. I thought there was something wrong with my code, but that wasn‚Äôt it - ElasticSearch recommends at least 4gb of RAM, but my server had 1 üòÇ. I needed to optimize for load, but tuning ElasticSearch indices seemed like a painful proposition. So, I instead opted to use Mongodb, which happily handled searching my metadata. Then, users complained about how the bot was super slow. To optimize for latency, I benchmarked a few solutions and ultimately just wrote an engine that would go through chat history to find relevant files. This was a significant speedup for search on recent files, but much slower for extremely old files. I was going to invest in a more complex solution, but users didn‚Äôt complain about the speed of search (which told me that users generally just need to search recent files).</p>

<p>Around the same time, I read through Discord‚Äôs engineering blogs on <a href="https://discord.com/blog/how-discord-indexes-billions-of-messages">search</a> and <a href="https://discord.com/blog/how-discord-stores-trillions-of-messages">storage</a>. They drove decisions about maintaining elastic search indices for search using discussions around latency metrics, storage architecture, etc. Their approach seemed disciplined, but I had no idea how to emulate it on a smaller budget. Still, it was a good example of the kind of engineering philosophy required to build at ‚Äúlarge-scale‚Äù. Reflecting on the experience showed me a lot of what I‚Äôd learned about software engineering in trying to scale my bot. I learned to approach hardware limitations, research others‚Äô approaches and combine them with my own ideas to create a solution that fit my constraints.</p>

<hr />

<p>When I asked my friend for this advice, I was halfway through my SWE internship at Snowflake on the applied machine learning team. My project was focused around building the first MVP of a clustering function for Snowflake‚Äôs <a href="https://docs.snowflake.com/en/guides-overview-ml-powered-functions">Cortex ML functions</a>. In the first half of my internship, I built a wrapper around <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">scikit-learn‚Äôs k-Means clustering</a>. In the second half, I focused on improving the latency and accuracy of my system. I also worked on an efficient algorithm to <a href="https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set">determine the number of clusters in a dataset</a>, which is what I want to focus on for this post.</p>

<h2 id="trying-to-estimate-the-number-of-clusters-in-a-dataset">Trying to estimate the number of clusters in a dataset</h2>

<p>When I began my work on methods to estimate the number of clusters in a dataset, I formulated a simple plan based on past research experience:</p>

<ol>
  <li>Conduct a literature review to evaluate existing methods for determining the number of clusters in a dataset. Try to answer some of these questions:
    <ul>
      <li>what is <em>necessary</em> to accomplish the task we have in mind?</li>
      <li>what is the state of the art (SOTA) method? Is the SOTA method tractable given our load and resources?</li>
      <li>what is the latency and efficacy of a particular method?</li>
    </ul>
  </li>
  <li>Choose and implement the best method.
    <ul>
      <li>Implement several approaches and iterate until you find an approach that works well</li>
      <li>Notice that this will typically bring you back to step 1, but there should be fewer things for you to research with every iteration</li>
    </ul>
  </li>
</ol>

<p><em>I‚Äôm always looking to learn more about effective research. If you have any suggestions, please share them (dhrumilp15[at]gmail[dot]com)!</em></p>

<p>Unfortunately, in step 1, I would read papers and articles explaining one particular technique, then compare it against the papers and articles explaining another technique, and notice that each technique had its own strengths and drawbacks. But, I wanted to find the ‚Äúbest‚Äù technique, some sort of holy grail that would always find the right number of clusters but was still tractable for massive customer datasets (spoiler: this doesn‚Äôt exist, at least according to my research). Even worse, I didn‚Äôt want to start implementing anything until I‚Äôd decided on the algorithm and approach I wanted to take. So, in order to find the best possible approach, I would compare minute details of algorithms, like whether generating reference distributions from the uniform distribution vs. randomly shuffling features of the data produced a more correct estimate when using the <a href="https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_gap_statistics">Gap Statistic</a>. I would try to combine multiple metrics to produce a better estimate, like whether we could try to find agreement between the <a href="https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index">Davies-Bouldin Index</a> and <a href="https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index">Calinski-Harabasz Score</a> for the correct number of clusters. Throughout this exploration, I <em>hadn‚Äôt written much code</em>. I was debating methods based on notions of the datasets, but didn‚Äôt have any hard numbers to back it up. This sunk a catastrophic amount of time, because I couldn‚Äôt commit to any one approach. I had fallen prey to <em>analysis paralysis</em>.</p>

<p>In previous internships, my teammates typically shook me out of analysis paralysis by performing the tough task of choosing an approach for me. I‚Äôd present all of my analysis, and let them choose the ‚Äúbest‚Äù approach. I trusted their judgement, and they generally commented that the approach was more than enough for now and that we should start implementing üòä.</p>

<p>In my time at Snowflake, I was shaken out of analysis paralysis by my performance evaluation, which commented that I needed to take a more active stance in my project and improve my productivity. When I heard the feedback, I was a little bit surprised, but not because I thought that the feedback was wrong. I realized that the feedback could be traced back to a somewhat simple cause: I just wasn‚Äôt communicating enough. I didn‚Äôt share what I was doing, my progress or what I envisioned for the future of the project.</p>

<p>I‚Äôd been following a working style from a previous internship, where some of the engineering advice I got was counter-productive, so I learned to show finished tasks rather than in-progress work. My performance evaluation showed me that I‚Äôd learned categorically wrong lessons - always share your work and agree on a plan to resolve engineering problems with new advice. This doesn‚Äôt mean that you must follow others‚Äô plans over your own, but you should at least communicate about what you believe to be the best path forward so that you can discover better paths or keep everyone on the same page about why you‚Äôre doing whatever it is you‚Äôre doing.</p>

<p>I‚Äôd also sunk a lot of time in my floundering with different approaches for estimating the number of clusters in a dataset. I realized that, like previous internships, if I shared my in-progress analysis with my mentor, we‚Äôd be able to discuss my results and decide on a plan for development.</p>

<p>Spurred by my performance evaluation, I rapidly implemented multiple approaches and tested their efficacy on sample datasets. After seeing disappointing performance from multiple algorithms (including the SOTA algorithm), it finally dawned on me that a perfect approach didn‚Äôt exist. I was genuinely crushed btw, I felt like someone had played a cruel joke on me and recorded my downfall for the world to see.</p>

<p>But, I‚Äôd made far more progress understanding the problem after implementing the approaches, which showed me that research and implementation need to go hand-in-hand. Anytime I compared two different approaches, in the lack of any other easily accessible information, the easiest and often fastest way to decide between them is to just implement both and see the results. In other words, to just</p>

<figure style="text-align: center">
<img src="https://tryitands.ee/tias_thumb.jpg" width="100%" height="auto" />
<figcaption>Try it and see! <a href="https://tryitands.ee" target="_blank">Original source</a></figcaption>
</figure>

<p>Implementing multiple approaches also showed me that my internal debates about minute algorithmic details were useless from both a research and practical standpoint. From a research standpoint, I was chasing options that were the least likely to produce sizeable benefits. Through implementation, you can focus your efforts to get the biggest gains upfront, and then later explore overhauls.</p>

<p>Practically, I can guess at the number of clusters forever, but customers will undoubtedly test multiple values. They would also much rather have new features built on top of the clustering function instead of a marginally more accurate estimate for the number of clusters.</p>

<h1 id="does-try-it-and-see-beat-analysis-paralysis">Does ‚ÄúTry it and see‚Äù beat analysis paralysis?</h1>

<p>No. The truth is, implementation can get you out of analysis paralysis (e.g. lit. review hell), but blindly using <em>try it and see</em> will just have you further digging in a bottomless rabbit hole. You have to construct limits around <em>try it and see</em>, because you ultimately need to break free from endless exploration and commit to exploiting one particular approach.</p>

<p>I found a surprisingly simple way to combat analysis paralysis when I talked to the caricature artist for the office party. He would draw caricatures in 10 minutes, and the timeout taught him the most about confidence, developing his own style and not thinking too hard about small details. His words reminded me of all of the doubt I‚Äôd been carrying in my work. His timeout meant that he couldn‚Äôt afford to agonize over every brush stroke, just like how I couldn‚Äôt afford to agonize over every particular algorithm.</p>

<p>Timeouts are also really common in the realm of games, where you would also encounter analysis paralysis:</p>
<ul>
  <li>you could analyze a chess board forever, but a blitz game requires that you do it around 30-40 times in 5 minutes or less. You can‚Äôt afford to spend all your time analyzing one position, which means that you need to make decisions and adapt.</li>
  <li>other Avalon players will jump on you from a great height if you take forever to choose a team for the mission.</li>
</ul>

<p><strong>Try it and see</strong> was my method of escaping the lit. review phase of analysis paralysis, but I fell into a similar cycle while experimenting with different metrics. I would try tweaking several different clustering quality metrics (Davies-Bouldin score, Calinski-Harabasz score, etc.) to see if any of them would produce better results. Instituting a timeout forced me to just choose what seemed to work the best, and then move on to implement the rest of the project. If we found a better method later, we would just replace a component.</p>

<h1 id="have-i-heard-this-before">Have I heard this before?</h1>

<p><em>Try it and see</em> is not a novel or unique idea. In writing this post, I thought about the advice I‚Äôve heard before, and how it encapsulates very similar ideas:</p>

<ul>
  <li>‚ÄúDon‚Äôt wait to gain the skills or knowledge you need to achieve what you want, they will come during the process‚Äù</li>
  <li>‚ÄúThe quality of founders is often determined not by the correctness of their decisions, but by how fast they can make them‚Äù</li>
  <li>‚ÄúThe best research students are those who can convert an idea to implementation really fast‚Äù</li>
</ul>

<p>Ultimately, these are effectively the same idea, and I‚Äôve heard them countless times. I then wonder: why has it taken me so long to internalize these ideas even a little bit? It makes me wonder: is advice not advice until you can say the same thing yourself?</p>

<p>I‚Äôve heard ‚Äúdo what you love‚Äù too many times to count, but I don‚Äôt think I truly understand it - how do you balance ‚Äúdo what you love‚Äù with familial obligations, or a failing economy? It seems like advice doesn‚Äôt actually become advice until I learn how to apply it, which tends to be when I can finally understand its message and say the same thing to someone else. So, <em>try it and see</em>.</p>

    </div>
    <div id='bibliography'>
        <div class='wrap'>
            <ol class="bibliography"></ol>
        </div>
    </div>
</div>
</body>
</html>